[2025-09-24 11:27:50][INFO][my_train_logger] DATASET:
  TEST:
    CACHE_IN_MEMORY: False
    NAME: StackOverflowDataset
    ROOT: /data/lj/task/trainlab/data
    TRANSFORM: None
  TRAIN:
    CACHE_IN_MEMORY: False
    NAME: StackOverflowDataset
    ROOT: /data/lj/task/trainlab/data
    TRANSFORM: None
  VAL:
    CACHE_IN_MEMORY: False
    NAME: StackOverflowDataset
    ROOT: /data/lj/task/trainlab/data
    TRANSFORM: None
MODEL:
  DROPOUT: 0.1
  EMB_SIZE: 768
  INTERMEDIATE_SIZE: 3072
  LAYER_NORM_EPS: 1e-12
  MAX_SEQ_LENGTH: 512
  NAME: BertForSequenceClassification
  N_CLASSES: 2
  N_HEADS: (12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12)
  N_LAYERS: 12
  PAD_TOKEN_ID: 103
  PRETRAINED: /data/lj/task/BERT-LoRA-TensorRT/bert-base-uncased
  RETURN_POOLER_OUTPUT: False
  VOCAB_SIZE: 30522
TRAINER:
  EPOCHS: 1
  LOSS_FN: CrossEntropyLoss
  NAME: BERTTrainer
  OPTIMIZER_CLASS: AdamW
  OPTIMIZER_KWARGS:
    LR: 5e-05
    WEIGHT_DECAY: 0.01
  OUTPUT_DIR: ./
  OUTPUT_FILENAME: weight
  SAVE: True
  SCHEDULER_CLASS: StepLR
  SCHEDULER_KWARGS:
    GAMMA: 0.95
    STEP_SIZE: 1
